Robot Motion Prediction Based on Scene Parsing in Night Mode

Abstract
The ability of scene recognition and autonomous motion of robot directly affects the task execution ability of robot. Automatic driving robot have the potential to transform the way we travel. The development of self driving robots is at a pivotal point, as a growing number of industrial and academic research organizations are bringing these technologies into a more complex and dangerous environment, such as nighttime. In order to enhance the robot 's understanding of the scene of night vision image, so it can detect and identify the surrounding environment faster and more accurately in the night mode, and realize the motion prediction of the robot. A method of motion prediction based on scene parsing is proposed. Through the observation of monocular infrared camera to learn to predict the future distribution of robot movement. The robot movement problem is divided into angle and speed control, Learn a control strategy that mimics people's choice of actions based on demonstrations of the desired behavior. Our model adopts a novel architecture, adding scene segmentation tasks to motion prediction tasks to improve performance, which realizes the environment perception and automatic driving of robot at night driving.
Index Terms-Motion prediction, scene parsing, infrared image, deep learning.

1. Introduction
Robots have strong environmental awareness, autonomous planning and adaptive capabilities. It can complete the specified work in a complex environment [1], have enjoyed considerable success in many applications such as search and rescue, monitoring, research, exploration, or mapping. 
Although the robot is running more and more autonomous, the recognition of the scene is still very critical. For robots with very limited payloads it is infeasible to carry state-of-the-art radars [1]. Many impressive advances have recently been made using laser range finders (lidar) [2-4] or Microsoft Kinect cameras (RGB-D sensors) [5]. Both sensors have a larger volume and power, which leads to increased power consumption and decreased operation time. In contrast, Vision-based robot navigation solutions are feasible [6-8]. Robots use the camera as a visual sensor to get the scene content, and then rely on the machine vision algorithm to understand the scene, to achieve autonomous navigation.
In the past few years, although the latest advances in rule-based approaches have achieved some success, the current system is only applicable to daytime or lighting conditions. Therefore, we believe that learning-based approaches will be ultimately needed to handle complex or atrocious scenes, such as night light conditions.
Based on this goal, we use a new infrared dataset and propose a novel deep learning architecture, adding scene parsing task to motion prediction task to better train the robot's driving strategy. Visual features extracted from the corresponding image are mapped to the control input provided by the human. Our frame can output the speed and angle of the control robot, as well as the night vision scene. Finally, our experimental results confirm that multi-task training with scene segmentation loss is better than the training of individual motion prediction task loss.
The rest of this letter is organized as follows: we next review the related work of robot motion prediction; we then discuss the infrared image dataset and its label information; we introduce a novel deep driving network; we report test results using the Infrared image dataset and deep driving networks; and finally, we present ideas for future work.

2. Related Work
The most popular sensors to carry robots are laser range finders and RGB-D sensors, as both deliver quite accurate depth estimates at a high framerate. Bachrach et al. [2] demonstrated using scanning lidars for Simultaneous Localization and Mapping (SLAM) in unknown indoor environments. The trend in indoor active sensing has been to use RGB-D sensors [5] that allow more detailed and faster scans. However, in outdoor environments, RGB-D sensors are often not applicable or suffer from very limited range. Therefore, Vandapel et al. [15] proposed outdoor planning approaches in three dimensions for UAV navigation using lidar data. For carrying outdoor lidar systems and the corresponding power supplies, larger and more expensive robots than what we aim for are required. Accurate depth estimation and localization is also possible with visual sensors using stereo cameras [16] or monocular camera [17]. A single camera is enough to create sparse [11] or even dense maps [8] of the environment. While such structure-from-motion techniques are already reasonably fast, they are still too computationally expensive for real-time requirements. Additionally, pure forward motion leads to an ill-posed problem when triangulating 3D points, because the triangulation angle is very small and the resulting position uncertainty is large.
Michels et al. [22] demonstrated driving a remote-controlled toy car outdoors using monocular vision and reinforcement learning. Hadsell et al. [23] showed in the LAGR project how to recognize and avoid obstacles within complex outdoor environments using vision and deep hierarchical networks. Bill et al. [24] used the often orthogonal structure of indoor scenes to estimate vanishing points and navigate a MAV in corridors by going towards the dominant vanishing point. 
Most closely related to our approach are approaches which learn motion policies from input data. ALVINN [16] was among the very first attempts to use a neural network for autonomous vehicle navigation. The approach was simple, comprising a shallow network that predicted actions from pixel inputs applied to simple driving scenarios with few obstacles; nevertheless, its success suggested the potential of neural networks for autonomous navigation.Instead of directly learning to map from pixels to actuation, [2] proposed mapping pixels to pre-defined affordance measures, such as the distance to surrounding cars. This approach provides human-interpretable intermediate outputs, but a complete set of such measures may be intractable to define in complex, real-world scenarios. Concurrent approaches in industry have used neural network predictions from tasks such as object detection and lane segmentation as inputs to a rule-based control system [10].
Another line of work has treated motion prediction as a visual prediction task in which future video frames are predicted on the basis of previous frames. [20] propose to learn a driving simulator with an approach that combines a Variational Auto-encoder (VAE) [11] and a Generative Adversarial Network (GAN) [8]. There are examples of video prediction models being applied to driving scenarios [5, 14]. However, in many scenarios, video prediction is ill-constrained as preceding actions are not given as input the model. [15, 7] address this by conditioning the prediction on the model’s previous actions. 
At present, there are few researches on robot motion prediction in night mode at home and abroad. We employ a novel deep learning framework with the combination of convolutional network [13] and deconvolution network that realizes robots motion prediction in night mode. Our model has additional knowledge of the scene parsing at training motion prediction. This extra information helps training of a better model.

3. Deep Driving Network 
We first describe our overall paradigm for learning a driving model in night mode from large-scale driving behavior data set and infrared image data set, and then propose a novel architecture for learning a deep driving network.
3.1  Driving Models
We propose an approach to learn a driving policy from behaviors, and formulate the problem as predicting future feasible actions. Our driving model is defined the admissibility of which next motion is plausible given the current observed scene. Formally, a driving model F is a function defined as:
 
where s denotes states, a represents a potential motion action and measures the feasibility score of operating motion action a under the circumstance s.
Our model takes the current image captured by the robot as input, and compute a likelihood over future motion actions. We only consider discrete settings in this paper. For example, the motion action set A could be a set of coarse actions:
 
where
 
 
3.2  Discrete Motion Prediction
A driving model should have correct motion action predictions despite encountering complicated scenes such as an intersection, traffic light, and/or pedestrians. We consider the case of discrete motion actions. For direction, we define three actions: straight, left turn, right turn. For speed, we also define three actions: normal, accelerate, slow down. The task is defined as predicting the feasible actions in the next frame. Specifically, we have as ground truth the robot’s speed and its angular velocity between the current frame and the frame immediately following. We define the action turning right as the event of an angular speed larger than   and turning left as an angular speed less than  .Otherwise, if the acceleration is less than  , we define the action slow down. The slow down action aims to describe when the robot has to act in order to avoid a crash. 
In real world of driving, it’s more prevalent to go straight, compared to turn left or right. Thus the samples in the training set are highly biased toward going straight. Similar to [26], we use the weighted loss of different actions according to the inverse of their prevalence.
3.3  Convolution-Deconvolution Architecture
Our ultimate goal is to predict the distribution over feasible future actions, conditioned on the current information. Although we can learn the driving policy in an end-to-end fashion, through experiments, we found that extra loss can improve the accuracy. We propose a novel architecture for classification and scene parsing called CDNN. Our model integrates convolution network with deconvolution network which uses scene parsing as the extra supervision and is able to jointly train motion prediction task and pixel-level supervised task。 We show below improved performance using a learning paradigm which combines both task loss and semantic segmentation label side training.
Given a video frame input, a visual encoder can encode the visual information in a discriminative manner while maintaining the relevant spatial information. In our architecture, a deconvolution neural network [25, 6] is utilized to extract the visual representations. Convolutional networks consist of convolution layers and pooling layers, and the deconvolution network contains unpooling layers and deconvolution layers. The process of pooling and unpooling is shown in figure 1. The deconvolution network unpoolings its input feature maps using the memorized max-pooling indices from the corresponding feature maps. The unpooling feature maps are sparse and are then deconvolved to produce dense feature maps. The convolution layer connects some input excitations to a single output excitation, while the deconvolution  layer connects a single input excitation to some output excitations, as shown in figure 2. Clipping the boundaries of a feature map to maintain the size of the output map as the size of the unpooling layer. Similar to convolutional networks, different layers in the deconvolution network can capture different levels of information. The kernels in the lower layer tend to capture the overall shape of the object, and the kernels in the higher layer tend to capture some minutiae.
 
Fig.1 Pooling and unpooling
 
Fig.2 Deconvolution operation
Specifically, We take the VGG16[vgg] as base network, output classification of angle and speed through full connection layer and Softmax. At the same time, add a scene parsing branch behind the pooling5 layer, which will enforce the model to learn a more meaningful feature representation, as shown in figure 3. For example, models can learn from scene parsing tasks that roads, pedestrians and vehicles have a greater impact on motion prediction, while sky, architecture, trees or others are almost negligible. As we will show in the experiment section that even when scene parsing is not the ultimate goal, learning with scene parsing tasks can improve performance, especially when coercing a model to attend to small scene phenomena. Therefore, the CDNN architecture is superior compared to the usual CNN architecture, due to it containing richer spatial information.
 
Fig.3 Network structure
The motion prediction and scene segmentation tasks in this paper are all multi class problems, so the Softmax is used to predict the probabilities of each category. During training, we define a multi-task loss on each frame as 
 .
where   represents the motion prediction loss and   represents the scene parsing loss.
Suppose that is the input of Softmax, and   is the output of Softmax, so
 
where y is the class corresponding to the input sample  , , then the loss function of   is defined as:
 
Each training frame is labeled with a ground-truth class and a ground-truth scene parsing class. We use a multi-task loss L on each labeled frame to jointly train for classification and scene parsing. The weights of classification loss and semantic segmentation loss are equal.

4. Experiments
4.1  Experimental process
We design two methods: the Direct Prediction Approach and the Scene Parsing Approach, we take the ImageNet [18] pre-train both two methods’ VGG16 model. The Direct Prediction Approach  direct output predicted angle and speed and the Scene Parsing Approach using our proposed architecture, adding the deconvolution networks to the convolution network, and then output angle, speed and scene parsing result.
 
Fig.4  Infrared image
Table 1 Infrared image data set label
[turn left, slow down]	[straight, acceleration]	[straight, normal]	[straight, acceleration]
[straight, acceleration]	[straight, slow down]	[straight, slow down]	[turn right, slow down]
In the Direct Prediction Approach, the input labels are the categories of angle and speed corresponding to each image. As shown in Figure 4, we use the calibrated infrared image and its corresponding speed and angle to fine tune the base network, the input labels are shown in table 1. Finally, the model predicts the motion distribution of robot.
In the Scene Parsing Approach, the input labels are the categories of angle and speed corresponding to each image, and the scene segmentation tags for each image.  As shown in Figure 5, we use the calibrated infrared image, the corresponding speed and angle, and the scene segmentation tags to fine tune the base network, the input labels are shown in table 1. Finally, the model is used to predict the motion distribution of the robot. The experimental procedure is shown in figure 6.
 
Fig.5  Infrared image and scene parsing label image
 
Fig.6 Experimental process
4.2  Experimental results and analysis
As shown in Table 2, The Scene Parsing Approach achieves the better performance in  accuracy. These observations align well with our intuition that training on multi-task in an end-to-end fashion improves performance. Figure 7 shows the prediction results of the two methods. The two methods give good decisions based on the current environment, for example, when there is no obstacle ahead, the robot can speed up and slow down in the corner. In the third column, there is a pedestrian in front of the robot, the Scene Parsing Approach successfully identifies it and predicts the speed as "slow down", while the Direct Prediction Approach predicts "normal".
 
Fig.7 Experiment result
Accuracy rate is a measure of the correct rate of classification. Let   be the prediction category for the ith sample, which   is the real category, and the accuracy of n test sample is
 
Where 1(x) is indicator function. When the predicted label is the same as the real label, it is 1. The accuracy is lower when the predicted labels is less consistent with the real labels. The accuracy rate has a wide range of applications, and can be used for multi-class and multi-label problems.
Table 2 Comparison of the two methods
Method	Angle Accuracy	Speed Accuracy
The Direct Prediction Approach	77.53%	75.46%
The Scene Parsing Approach	83.96%	82.51%
The Direct Prediction Approach performs well in general, but occasionally misses important fine-grained details. Our Scene Parsing Approach provides a more semantically meaningful feature representation and has a good performance in complex environment.
5. Conclusion
In this paper, we introduce a generic egomotion prediction approach for deep visuomotor learning. We propose a novel CDNN architecture that can learn jointly from the scene parsing loss and the motion prediction loss, and can realize robot motion prediction in night mode. We have proven the usefulness of having a side task as the extra supervision. The model shows good performance on the driving task.
